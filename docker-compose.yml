# Bielik MVP - Docker Compose
# Uruchom: docker compose up -d
# Używa lokalnej Ollamy (na hoście), nie w Docker

services:
  # ============================================
  # LLM Backend - Ollama (WYŁĄCZONE - używamy lokalnej)
  # ============================================
  # ollama:
  #   image: ollama/ollama:latest
  #   container_name: bielik-ollama
  #   ports:
  #     - "${OLLAMA_PORT:-11434}:11434"
  #   volumes:
  #     - ./data/ollama:/root/.ollama
  #   environment:
  #     - OLLAMA_HOST=0.0.0.0
  #     - OLLAMA_KEEP_ALIVE=24h
  #   restart: unless-stopped
  #   healthcheck:
  #     test: ["CMD", "curl", "-f", "http://localhost:11434/"]
  #     interval: 30s
  #     timeout: 10s
  #     retries: 3
  #   # ----------------------------------------
  #   # Dla GPU NVIDIA odkomentuj poniższe:
  #   # ----------------------------------------
  #   # deploy:
  #   #   resources:
  #   #     reservations:
  #   #       devices:
  #   #         - driver: nvidia
  #   #         count: all
  #   #         capabilities: [gpu]

  # ============================================
  # Baza danych - PostgreSQL z pgvector (external)
  # Używamy istniejącego kontenera bielik-postgres
  # ============================================
  # postgres:
  #   image: pgvector/pgvector:pg16
  #   container_name: bielik-postgres
  #   ... (wyłączone - używamy external)

  # ============================================
  # API Backend - FastAPI z RAG
  # ============================================
  api:
    build: 
      context: ./modules/api
      dockerfile: Dockerfile
    container_name: bielik-api
    environment:
      DATABASE_URL: postgresql://${POSTGRES_USER:-bielik}:${POSTGRES_PASSWORD:-bielik_dev_2024}@${POSTGRES_HOST:-bielik-postgres}:5432/${POSTGRES_DB:-bielik_knowledge}
      OLLAMA_URL: ${OLLAMA_URL:-http://172.17.0.1:11434}
      OLLAMA_MODEL: ${OLLAMA_MODEL:-llama3.1:8b}
    extra_hosts:
      - "host.docker.internal:172.17.0.1"
    ports:
      - "${API_PORT:-8000}:8000"
    volumes:
      - ./data:/app/data:ro
    restart: unless-stopped
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8000/health"]
      interval: 30s
      timeout: 10s
      retries: 3

  # ============================================
  # Frontend - Nginx ze statycznym HTML/JS
  # ============================================
  frontend:
    build:
      context: ./modules/frontend
      dockerfile: Dockerfile
    container_name: bielik-frontend
    depends_on:
      - api
    ports:
      - "${FRONTEND_PORT:-3000}:80"
    restart: unless-stopped

  # ============================================
  # Model Loader (WYŁĄCZONE - używamy lokalnej Ollamy)
  # ============================================
  # model-loader:
  #   image: curlimages/curl:latest
  #   container_name: bielik-model-loader
  #   depends_on:
  #     ollama:
  #       condition: service_started
  #   entrypoint: >
  #     sh -c "
  #       echo 'Czekam na Ollama...' &&
  #       sleep 15 &&
  #       echo 'Pobieram model Bielik...' &&
  #       curl -X POST http://ollama:11434/api/pull -d '{\"name\": \"mwiewior/bielik\"}' &&
  #       echo 'Model pobrany!'
  #     "
  #   restart: "no"

# volumes:
#   postgres_data:
#     name: bielik-postgres-data
#     external: true

networks:
  default:
    name: bielik-network
    external: true
